---
title: 机器学习 | 神经网络
date: 2018-10-09 09:12:04
tags:
---
神经网络介绍
<!-- more -->
## 神经元模型
如果某个神经元的电位超过了一个阈值，那么它就会被激活
M-P 神经元模型：神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总收入值将与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出。
激活函数：
理想：阶跃函数，但是不连续，不光滑。
常用：sigmod 函数（挤压函数）
## 感知机与多层网络
>**感知机**由两层神经元组成，输入层接收外界信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”

感知机只有输出层神经元进行激活函数处理，及只拥有一层功能神经元。  
隐层，隐含层和输出层神经元都是拥有激活函数的功能神经元。
>多层前馈神经网络：输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。
(还有训练递归神经网络)
神经网络就是学习连接权和阈值。

## BP 误差逆传播算法
一般BP网络指使用BP算法训练的多层前馈神经网络。
**目标**：最小化训练集D上的误差。  
**工作流程**：
先将输入示例提供给输入层神经元，然后逐层将信号前传，知道产生输出层的结果；然后计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整。

如果基于累积误差最小化的更新规则，就得到了**累计误差逆传播** ，即累计BP
对比：标准BP算法往往需进行更多次数的迭代，累计BP算法直接针对累计误差最小化。在训练集非常大时，标准BP往往会更快获得较好的解。
- BP过拟合问题
  - 早停
  - 正则化

## 全局最小和局部极小
基于梯度的搜索是使用最为广泛的参数寻优方法。
跳出局部极小，接近全局最小的方法
- 以多组不同参数值初始化多个神经网络，训练之后取其中误差最小的解作为最终参数
- 使用模拟退火
- 随机梯度下降
遗传算法也常用来训练神经网络以更好地逼近全局最小。

## 其他神经网络
### RBF 网络
径向基函数网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元的激活函数，而输出层则是对隐层神经元输出的线性组合。
### ART 网络
**竞争型学习**：无监督学习策略，网络输出神经元相互竞争，每一个时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。
**ART**（adaptive resonance theory 自适应振谐理论）网络 ：该网络由比较层，识别层，识别阈值和重置模块构成。

优点：缓解竞争型学习中的“可塑性-稳定性”窘境，可进行增量学习或在线学习

### SOM 网络
self-organization Map **自组织映射网络**是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构。
目的：每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的

### 级联相关网络
结构自适应网络的重要代表，将网络结构也作为训练目标
主要成分：级联和相关，级联是指建立层次连接的层级结构
特点：无需设置网络层数，隐层神经元数目，且训练速度较快，但其在数据较小时易陷入过拟合

### Elman 网络
**递归神经网络**，允许网络中出现环形结构，
隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入  
>**Elman 网络**：最常用的递归神经网络之一，隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。
### `Boltzmann` 机
基于能量的模型，能量最小化时网络达到理想状态。
神经元分为显层和隐层，显层用于把表示数据的输入与输出，隐层则被理解为数据的内在表达。  
神经元都是布尔型的。  
训练过程：将每个训练样本视为一个状态变量，其出现的概率尽可能大。
评价：标准的`Boltzmann` 机是全连接图，训练复杂度高，因此现实使用RBM，之保持显层与隐层之间的连接。常用**对比散度(CD)** 进行训练

## 深度学习
**多隐层网络训练**
**无监督逐层训练**是多隐层网络训练的有效手段。
深度信念网络：DBN 
**权共享**：节省训练开销，在卷积神经网络中发挥作用。
将深度学习理解为进行**特征学习**或**表示学习**
